{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662fcdd7",
   "metadata": {},
   "source": [
    "# æŒæ¡LCEL(LangChain Expression Language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf22d98",
   "metadata": {},
   "source": [
    "## æµ‹è¯•å¤§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0579c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/mydisk/home/chenxd/.conda/envs/lang/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤§æ¨¡å‹å›å¤ï¼š\n",
      " ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼ğŸ˜Š æˆ‘æ˜¯DeepSeekï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„AIåŠ©æ‰‹ã€‚æ— è®ºä½ æœ‰ä»€ä¹ˆé—®é¢˜ã€éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Œæˆ–è€…åªæ˜¯æƒ³èŠèŠå¤©ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›æ”¯æŒï¼\n",
      "\n",
      "æˆ‘å¯ä»¥å¸®ä½ è§£ç­”å„ç§é—®é¢˜ï¼ŒååŠ©å¤„ç†æ–‡æ¡£ï¼Œè¿›è¡Œåˆ›ä½œå’Œåˆ†æç­‰ç­‰ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ä¸ºä½ åšçš„å—ï¼Ÿæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½çƒ­æƒ…åœ°å¸®åŠ©ä½ ï¼âœ¨\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "def call_deepseek(prompt):\n",
    "    try:\n",
    "        # åˆå§‹åŒ– DeepSeek èŠå¤©æ¨¡å‹\n",
    "        model = ChatDeepSeek(\n",
    "            model=\"deepseek-chat\",  # æ¨¡å‹åç§°\n",
    "            api_key=os.getenv(\"DEEPSEEK_API_KEY\"),  # ä»ç¯å¢ƒå˜é‡è·å– API å¯†é’¥\n",
    "            api_base=os.getenv(\"DEEPSEEK_BASE_URL\"),  # ä»ç¯å¢ƒå˜é‡è·å– API åŸºç¡€åœ°å€\n",
    "            temperature=0.7,  # é‡‡æ ·æ¸©åº¦\n",
    "            max_tokens=1000  # æœ€å¤§ç”Ÿæˆ tokens\n",
    "        )\n",
    "        \n",
    "        # æ„é€ æ¶ˆæ¯åˆ—è¡¨ï¼ˆLangChain é€šå¸¸ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨æ ¼å¼ï¼‰\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        \n",
    "        # è°ƒç”¨æ¨¡å‹å¹¶è·å–ç»“æœ\n",
    "        response = model.invoke(messages)\n",
    "        \n",
    "        # è¿”å›ç”Ÿæˆçš„å†…å®¹\n",
    "        return response.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"è°ƒç”¨å¤±è´¥ï¼š{str(e)}\"\n",
    "\n",
    "# æµ‹è¯•è°ƒç”¨\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"ä½ å¥½\"\n",
    "    result = call_deepseek(prompt)\n",
    "    print(\"å¤§æ¨¡å‹å›å¤ï¼š\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f8cc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼ğŸ˜Š æˆ‘æ˜¯DeepSeekï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„AIåŠ©æ‰‹ã€‚æ— è®ºä½ æœ‰ä»€ä¹ˆé—®é¢˜ã€éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Œæˆ–è€…åªæ˜¯æƒ³èŠèŠå¤©ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›æ”¯æŒï¼\n",
      "\n",
      "æˆ‘å¯ä»¥å¸®ä½ å¤„ç†å„ç§ä»»åŠ¡ï¼Œæ¯”å¦‚ï¼š\n",
      "- å›ç­”é—®é¢˜å’Œè§£é‡Šæ¦‚å¿µ\n",
      "- ååŠ©å†™ä½œå’Œç¿»è¯‘\n",
      "- åˆ†æå’Œå¤„ç†æ–‡æ¡£\n",
      "- ç¼–ç¨‹å’ŒæŠ€æœ¯æ”¯æŒ\n",
      "- åˆ›æ„æ€è€ƒå’Œå¤´è„‘é£æš´\n",
      "\n",
      "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ä¸ºä½ åšçš„å—ï¼Ÿè¯·éšæ—¶å‘Šè¯‰æˆ‘ä½ çš„éœ€æ±‚ï¼âœ¨"
     ]
    }
   ],
   "source": [
    "def stream_deepseek(prompt):\n",
    "    model = ChatDeepSeek(\n",
    "        model=\"deepseek-chat\",\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        api_base=os.getenv(\"DEEPSEEK_BASE_URL\"),\n",
    "        temperature=0.7\n",
    "    )\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    for chunk in model.stream(messages):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "# æµå¼è°ƒç”¨æµ‹è¯•\n",
    "if __name__ == \"__main__\":\n",
    "    stream_deepseek(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32d366",
   "metadata": {},
   "source": [
    "## Model I/O ä¸‰å…ƒç»„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2220871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹è¾“å‡ºï¼š LangChain çš„ model I/O ä¸‰å…ƒç»„æ˜¯è¾“å…¥ï¼ˆPromptsï¼‰ã€æ¨¡å‹ï¼ˆModelsï¼‰å’Œè¾“å‡ºè§£æï¼ˆOutput Parsersï¼‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI  # DeepSeek å…¼å®¹ OpenAI æ¥å£\n",
    "\n",
    "# åŠ è½½ä½ çš„ .env\n",
    "load_dotenv()\n",
    "\n",
    "# æ›¿æ¢ä¸º DeepSeek çš„ API é…ç½®\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "base_url = os.getenv(\"DEEPSEEK_BASE_URL\")\n",
    "\n",
    "# 1) Modelï¼šDeepSeek æ¨¡å‹ï¼ˆä½¿ç”¨å…¼å®¹ OpenAI æ¥å£çš„æ–¹å¼ï¼‰\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",  # DeepSeek ä¸»æ¨çš„å¯¹è¯æ¨¡å‹\n",
    "    temperature=0.2,\n",
    "    api_key=api_key,\n",
    "    base_url=base_url,\n",
    "    max_tokens=1024,  # å¯é€‰ï¼šè®¾ç½®æœ€å¤§ç”Ÿæˆtokenæ•°\n",
    "    timeout=30,       # å¯é€‰ï¼šè®¾ç½®è¶…æ—¶æ—¶é—´\n",
    ")\n",
    "\n",
    "# 2) Promptï¼šèŠå¤©æç¤ºæ¨¡æ¿ï¼ˆä¿æŒä¸å˜ï¼ŒDeepSeek é€‚é…æ ‡å‡†çš„ ChatML æ ¼å¼ï¼‰\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä½ç®€æ´ã€å‡†ç¡®çš„ä¸­æ–‡åŠ©æ‰‹ã€‚\"),\n",
    "    (\"human\", \"è¯·ç”¨ä¸€å¥è¯å›ç­”ï¼š{question}\")\n",
    "])\n",
    "\n",
    "# 3) Output Parserï¼šå°†æ¨¡å‹è¾“å‡ºè§£æä¸ºå­—ç¬¦ä¸²\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# ä¸‰å…ƒç»„é“¾ï¼šPrompt â†’ Model â†’ Parser\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# è¿è¡Œç¤ºä¾‹\n",
    "inputs = {\"question\": \"LangChain çš„ model I/O ä¸‰å…ƒç»„æ˜¯ä»€ä¹ˆï¼Ÿ\"}\n",
    "result = chain.invoke(inputs)\n",
    "print(\"æ¨¡å‹è¾“å‡ºï¼š\", result)\n",
    "\n",
    "# å¯é€‰ï¼šæ‰¹é‡è°ƒç”¨ç¤ºä¾‹\n",
    "# questions = [\n",
    "#     {\"question\": \"LangChain çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ\"},\n",
    "#     {\"question\": \"ä»€ä¹ˆæ˜¯ Prompt Engineeringï¼Ÿ\"},\n",
    "#     {\"question\": \"LLM çš„ä¸Šä¸‹æ–‡çª—å£æ˜¯ä»€ä¹ˆï¼Ÿ\"}\n",
    "# ]\n",
    "# for input_data in questions:\n",
    "#     print(f\"\\né—®é¢˜ï¼š{input_data['question']}\")\n",
    "#     print(f\"å›ç­”ï¼š{chain.invoke(input_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d22a4d",
   "metadata": {},
   "source": [
    "## Runnableå¯¹è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a4927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain çš„ Runnable æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–æ¥å£ï¼Œç”¨äºç»Ÿä¸€è°ƒç”¨é“¾ã€å·¥å…·ã€æ¨¡å‹ç­‰ç»„ä»¶ï¼Œç®€åŒ–æ„å»ºå’Œç»„åˆ AI åº”ç”¨çš„è¿‡ç¨‹ã€‚\n",
      "Runnable çš„æ ¸å¿ƒèƒ½åŠ›åŒ…æ‹¬ï¼šä»»åŠ¡ç¼–æ’ã€å·¥å…·è°ƒç”¨å’Œç»Ÿä¸€æ¥å£ã€‚\n",
      "Runnable æ”¯æŒé€šè¿‡ invoke æ‰§è¡Œå•æ¬¡è°ƒç”¨ã€batch å¤„ç†æ‰¹é‡ä»»åŠ¡ä»¥åŠ stream å®ç°æµå¼å“åº”ã€‚\n",
      "Runnable é€šè¿‡ç®¡é“ç¬¦ | è½»æ¾å®ç°ç»„ä»¶ä¸²è”ã€å¹¶è¡Œä¸æ¡ä»¶ç»„åˆã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "base_url = os.getenv(\"DEEPSEEK_BASE_URL\")\n",
    "\n",
    "# æ¨¡å‹ï¼šdeepseek-chatï¼ˆä½¿ç”¨ DeepSeek çš„æ¥å£ï¼‰\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=api_key,\n",
    "    base_url=base_url,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# Promptï¼šç®€å•èŠå¤©æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä½ç®€æ´ã€å‡†ç¡®çš„ä¸­æ–‡åŠ©æ‰‹ã€‚\"),\n",
    "    (\"human\", \"è¯·ç”¨ä¸€å¥è¯å›ç­”ï¼š{question}\")\n",
    "])\n",
    "\n",
    "# è§£æå™¨ï¼šå°†æ¶ˆæ¯è§£æä¸ºçº¯å­—ç¬¦ä¸²\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# ä¸‰å…ƒç»„é“¾ï¼šPrompt â†’ Model â†’ Parser\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# 1) ç›´æ¥è°ƒç”¨ invoke\n",
    "print(chain.invoke({\"question\": \"ç”¨ä¸€å¥è¯è§£é‡Š LangChain çš„ Runnableã€‚\"}))\n",
    "\n",
    "# 2) ä½¿ç”¨ RunnableLambda åšè¾“å…¥å˜æ¢ï¼Œå†æ¥ä¸‰å…ƒç»„é“¾\n",
    "to_question = RunnableLambda(lambda x: {\"question\": f\"è¯·åˆ†ä¸‰ç‚¹æ¦‚æ‹¬ï¼š{x['topic']}\"})\n",
    "chain2 = to_question | prompt | llm | parser\n",
    "\n",
    "print(chain2.invoke({\"topic\": \"Runnable çš„æ ¸å¿ƒèƒ½åŠ›\"}))\n",
    "\n",
    "# 3) æ‰¹é‡è°ƒç”¨ batch\n",
    "inputs = [\n",
    "    {\"topic\": \"Runnable æ”¯æŒ invoke/batch/stream\"},\n",
    "    {\"topic\": \"Runnable é€šè¿‡ | è½»æ¾ç»„åˆç»„ä»¶\"},\n",
    "]\n",
    "for out in chain2.batch(inputs):\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae14a0",
   "metadata": {},
   "source": [
    "## é€šè¿‡ Runnable è¾“å‡ºé“¾è·¯ ASCII å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f25258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/mydisk/home/chenxd/.conda/envs/lang/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "  +--------------------+   \n",
      "  | ChatPromptTemplate |   \n",
      "  +--------------------+   \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOpenAI |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ç›´æ¥åŠ è½½å½“å‰ç›®å½•ä¸‹çš„ .env\n",
    "load_dotenv()\n",
    "\n",
    "# è¯»å– DeepSeek ç¯å¢ƒå˜é‡\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "base_url = os.getenv(\"DEEPSEEK_BASE_URL\")\n",
    "\n",
    "# æ¨¡å‹ï¼šDeepSeek çš„ OpenAI å…¼å®¹æ¥å£\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=api_key,\n",
    "    base_url=base_url,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# Promptï¼šç®€å•èŠå¤©æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä½ç®€æ´ã€å‡†ç¡®çš„ä¸­æ–‡åŠ©æ‰‹ã€‚\"),\n",
    "    (\"human\", \"è¯·ç”¨ä¸€å¥è¯å›ç­”ï¼š{question}\")\n",
    "])\n",
    "\n",
    "# è§£æå™¨ï¼šå°†æ¨¡å‹æ¶ˆæ¯è§£æä¸ºå­—ç¬¦ä¸²\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# ä¸‰å…ƒç»„é“¾ï¼ˆRunnableï¼‰ï¼šPrompt â†’ Model â†’ Parser\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# è¾“å‡ºé“¾è·¯ç»“æ„çš„ ASCII å­—ç¬¦å›¾\n",
    "chain.get_graph().print_ascii()\n",
    "\n",
    "# å¯é€‰ï¼šå®é™…è°ƒç”¨ä¸€æ¬¡é“¾ï¼ŒéªŒè¯é€»è¾‘ï¼ˆä¸æ˜¯å¿…é¡»ï¼‰\n",
    "# print(chain.invoke({\"question\": \"LangChain çš„ Runnable æ˜¯ä»€ä¹ˆï¼Ÿ\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
